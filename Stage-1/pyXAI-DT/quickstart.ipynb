{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95a492d4",
      "metadata": {
        "id": "95a492d4"
      },
      "source": [
        "# Quick Start"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLmc5TBvKhTM",
        "outputId": "b87137d1-737b-4691-a0d5-326997416b5c"
      },
      "id": "yLmc5TBvKhTM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80685d54",
      "metadata": {
        "id": "80685d54"
      },
      "source": [
        "{: .attention }\n",
        "> Once pyxai has been installed, you can use these commands:\n",
        ">\n",
        "> ```python3 <file.py>```: Execute a python file with lines of code using PyXAI\\\\\n",
        "> ```python3 -m pyxai -gui```: Open the PyXAI's Graphical User Interface\\\\\n",
        "> ```python3 -m pyxai -explanations```: Copy the explanations backups of GUI in your current directory\\\\\n",
        "> ```python3 -m pyxai -examples```: Copy the examples in your current directory\n",
        "\n",
        "Let us give a quick illustration of PyXAI, showing how to compute explanations given a ML model.\n",
        "<img src=\"attachment:irislatex.png\" alt=\"Iris\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f898fcd6",
      "metadata": {
        "id": "f898fcd6"
      },
      "source": [
        "The first thing to do is to import the components of PyXAI. In order to import only the necessary methods into a project, PyXAI is composed of three distinct modules: ```Learning```, ```Explainer```, and ```Tools```."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyxai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soqPUCXzLAZp",
        "outputId": "2edb8e14-3eda-47aa-843e-66adcd0a4ade"
      },
      "id": "soqPUCXzLAZp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/643.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/643.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/643.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m634.9/643.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.4/643.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docplex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypblib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1500590b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1500590b",
        "outputId": "02a64a67-0501-4ac5-c514-237c45e91b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from pyxai import Learning, Explainer, Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4d6513",
      "metadata": {
        "id": "ca4d6513"
      },
      "source": [
        "If you encounter a problem, this is certainly because you need the python package PyXAI to be installed on your system. You need to execute a command like ```python3 -m pip install pyxai```. See the [Installation](/documentation/installation) page for details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dff254a",
      "metadata": {
        "id": "8dff254a"
      },
      "source": [
        "In most situations, the use of PyXAI library requires to achieve two successive steps: first the generation of an ML model from a dataset (with the ```Learning``` module) and second, given the learned model, the computation of explanations for some instances (using the ```Explainer``` module)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5714f78",
      "metadata": {
        "id": "a5714f78"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa225621",
      "metadata": {
        "id": "aa225621"
      },
      "source": [
        "For this example, we want to create a decision tree classifier for the iris dataset using [Scikit-learn](https://scikit-learn.org/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_dataset_path = \"/content/drive/MyDrive/PROJETO - THIAGO ALVES /pyxai/iris.csv\""
      ],
      "metadata": {
        "id": "vBG3S5H6Nsz4"
      },
      "id": "vBG3S5H6Nsz4",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "bc48a391",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc48a391",
        "outputId": "031b45fe-6ecc-4f86-f5ae-904c707b4505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data:\n",
            "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width         Species\n",
            "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
            "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
            "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
            "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
            "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
            "..            ...          ...           ...          ...             ...\n",
            "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
            "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
            "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
            "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
            "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "--------------   Information   ---------------\n",
            "Dataset name: /content/drive/MyDrive/PROJETO - THIAGO ALVES /pyxai/iris.csv\n",
            "nFeatures (nAttributes, with the labels): 5\n",
            "nInstances (nObservations): 150\n",
            "nLabels: 3\n"
          ]
        }
      ],
      "source": [
        "learner = Learning.Scikitlearn(file_dataset_path, learner_type=Learning.CLASSIFICATION) # modifique o caminho para o seu dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12124c7",
      "metadata": {
        "id": "d12124c7"
      },
      "source": [
        "It is possible to download this dataset from the [UCI Machine Learning Repository -- Iris Data Set](http://archive.ics.uci.edu/ml/datasets/Iris) or [here](/assets/notebooks/dataset/iris.csv). In our case, it is located in the directory ```../dataset```. The parameter ```learner_type=Learning.CLASSIFICATION``` asks to achieve a classification task. The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). The goal of the classifier is to find the right outcome for an instance among three classes: setosa, virginica, versicolor.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54c678e",
      "metadata": {
        "id": "d54c678e"
      },
      "source": [
        "To create models, PyXAI implements methods to directly run an ML experimental protocol (with the train-test split technique). Several cross-validation methods (```Learning.HOLD_OUT```, ```Learning.K_FOLDS```, ```Learning.LEAVE_ONE_GROUP_OUT```) and models (```Learning.DT```, ```Learning.RF```, ```Learning.BT```) are available.\n",
        "\n",
        "In this example, we compute a Decision Tree (see the parameter ```output=Learning.DT```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0c692233",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c692233",
        "outputId": "e93ff4a2-0d0c-411b-c90a-f7d19172079f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------   Evaluation   ---------------\n",
            "method: HoldOut\n",
            "output: DT\n",
            "learner_type: Classification\n",
            "learner_options: {'max_depth': None, 'random_state': 0}\n",
            "---------   Evaluation Information   ---------\n",
            "For the evaluation number 0:\n",
            "metrics:\n",
            "   micro_averaging_accuracy: 98.51851851851852\n",
            "   micro_averaging_precision: 97.77777777777777\n",
            "   micro_averaging_recall: 97.77777777777777\n",
            "   macro_averaging_accuracy: 98.51851851851853\n",
            "   macro_averaging_precision: 97.22222222222221\n",
            "   macro_averaging_recall: 98.14814814814815\n",
            "   true_positives: {'Iris-setosa': 16, 'Iris-versicolor': 17, 'Iris-virginica': 11}\n",
            "   true_negatives: {'Iris-setosa': 29, 'Iris-versicolor': 27, 'Iris-virginica': 33}\n",
            "   false_positives: {'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 1}\n",
            "   false_negatives: {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 0}\n",
            "   accuracy: 97.77777777777777\n",
            "   sklearn_confusion_matrix: [[16, 0, 0], [0, 17, 1], [0, 0, 11]]\n",
            "nTraining instances: 105\n",
            "nTest instances: 45\n",
            "\n",
            "---------------   Explainer   ----------------\n",
            "For the evaluation number 0:\n",
            "**Decision Tree Model**\n",
            "nFeatures: 4\n",
            "nNodes: 6\n",
            "nVariables: 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = learner.evaluate(method=Learning.HOLD_OUT, output=Learning.DT)  # no output é onde define se é um modelo de DF ou RF por exemplo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e45e421",
      "metadata": {
        "id": "1e45e421"
      },
      "source": [
        "Uma vez criado o modelo, selecionamos uma instância para poder derivar explicações. Aqui, uma instância bem classificada é escolhida: o modelo prevê a primeira classe ```0``` (ou seja, a classe Iris setosa) graças ao ```correct=True``` e ao ```predictions=[ 0]``` parâmetros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "2577c3ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2577c3ec",
        "outputId": "3ef1ebea-26d1-4a0c-b5bc-53d8a0e4f8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------   Instances   ----------------\n",
            "number of instances selected: 2\n",
            "----------------------------------------------\n",
            "(array([5.1, 3.5, 1.4, 0.2]), 0)\n",
            "\n",
            " (array([4.9, 3. , 1.4, 0.2]), 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.1, 3.5, 1.4, 0.2])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "instance, prediction = learner.get_instances(model, n=2, correct=True, predictions=[0])\n",
        "\n",
        "# n = quantidade de instancias que a gente quer daquela classe predictions0\n",
        "\n",
        "print(instance)\n",
        "print('\\n', prediction)\n",
        "instance[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padrao\n",
        "instance, prediction = learner.get_instances(model, n=1, correct=True, predictions=[0])\n",
        "\n",
        "# n = quantidade de instancias que a gente quer daquela classe predictions0\n",
        "\n",
        "print(instance)\n",
        "print('\\n', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctjrGTWxSsc6",
        "outputId": "652f5766-108c-42ce-8910-8623bf7193f0"
      },
      "id": "ctjrGTWxSsc6",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------   Instances   ----------------\n",
            "number of instances selected: 1\n",
            "----------------------------------------------\n",
            "[5.1 3.5 1.4 0.2]\n",
            "\n",
            " 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5372cec4",
      "metadata": {
        "id": "5372cec4"
      },
      "source": [
        "Please consult the [Learning](/documentation/learning/generating) page for more details about this ML part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b4ec8b",
      "metadata": {
        "id": "23b4ec8b"
      },
      "source": [
        "## Explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4a98b8",
      "metadata": {
        "id": "5b4a98b8"
      },
      "source": [
        "O módulo ```Explainer``` contém diferentes métodos para gerar explicações. Para isso, o modelo e a instância alvo são definidos como parâmetros da função ```initialize``` deste módulo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "7d7e859c",
      "metadata": {
        "id": "7d7e859c"
      },
      "outputs": [],
      "source": [
        "explainer = Explainer.initialize(model, instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5257e4b0",
      "metadata": {
        "id": "5257e4b0"
      },
      "source": [
        "A função ```initialize``` converte a instância em variáveis ​​binárias (chamadas de representação binária) codificando o modelo associado. Mais precisamente, cada uma dessas variáveis ​​binárias representa uma condição (característica $op$ valor?) no modelo onde $op$ é um operador de comparação padrão. [Scikit-learn](https://scikit-learn.org/stable/) e [XGBoost](https://xgboost.readthedocs.io/en/stable/) usam o operador $\\ge$. Com relação à instância, o sinal de uma variável binária indica se a condição é verdadeira ou não no modelo. Aqui podemos ver a instância e sua representação binária. Podemos ver as condições relacionadas à representação binária usando a função ```to_features``` que é explicada abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "4ce31e13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce31e13",
        "outputId": "922aba7a-26ae-4cc6-c9a7-47870960d2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instance: [5.1 3.5 1.4 0.2]\n",
            "binary representation: (-1, -2, -3, 4, -5)\n",
            "conditions related to the binary representation: ('Sepal.Width > 3.100000023841858', 'Petal.Length <= 4.950000047683716', 'Petal.Width <= 0.75', 'Petal.Width <= 1.6500000357627869', 'Petal.Width <= 1.75')\n",
            "conditions related to the binary representation: ('Sepal.Width > 3.100000023841858', 'Petal.Length <= 4.950000047683716', 'Petal.Width <= 0.75')\n"
          ]
        }
      ],
      "source": [
        "print(\"instance:\", instance)\n",
        "print(\"binary representation:\", explainer.binary_representation)\n",
        "print(\"conditions related to the binary representation:\", explainer.to_features(explainer.binary_representation,eliminate_redundant_features=False))\n",
        "print(\"conditions related to the binary representation:\", explainer.to_features(explainer.binary_representation,eliminate_redundant_features=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28df2c0f",
      "metadata": {
        "id": "28df2c0f"
      },
      "source": [
        "Notamos que a representação binária desta instância contém mais de 4 variáveis ​​porque a árvore de decisão do modelo é composta por cinco nós (variáveis ​​binárias). Na verdade, o recurso Petal.Width aparece 3 vezes, enquanto Sepal.Length é inútil. Consulte a página [conceitos](/documentation/explainer/concepts/) para obter mais informações sobre representações binárias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62766926",
      "metadata": {
        "id": "62766926"
      },
      "source": [
        "### Abductive explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea0e27bc",
      "metadata": {
        "id": "ea0e27bc"
      },
      "source": [
        "In PyXAI, several types of explanation are available. In their binary forms representing conditions, these are called reasons. In our example, we choose to compute one of the most popular type of explanations: a sufficient reason. A sufficient reason is an abductive explanation (any other instance X' sharing the conditions of this reason is classified by the model as X is) for which no proper subset of this reason is a sufficient reason (i.e., the explanation is minimal with respect to set inclusion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "3929b134",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3929b134",
        "outputId": "f6beeb9c-dc98-4aa1-b096-f8fe7a0c88a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sufficient_reason: (-1,)\n"
          ]
        }
      ],
      "source": [
        "sufficient_reason = explainer.sufficient_reason(n=1)\n",
        "print(\"sufficient_reason:\", sufficient_reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e5f6bc",
      "metadata": {
        "id": "b8e5f6bc"
      },
      "source": [
        "We can get the features involved in the reason thanks to the method ```to_features```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "b297f93b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b297f93b",
        "outputId": "5969ef5d-efbb-42d3-ee4b-5e2a683055fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to_features: ('Petal.Width <= 0.75',)\n"
          ]
        }
      ],
      "source": [
        "print(\"to_features:\", explainer.to_features(sufficient_reason))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do método `to_features` com o parâmetro `details`\n",
        "detailed_features = explainer.to_features(sufficient_reason, details=True)\n",
        "\n",
        "# Imprime a saída detalhada\n",
        "\"Detailed features: \", detailed_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceADCyGmZBwr",
        "outputId": "350538a9-8521-4fe4-975a-9a6a925d1403"
      },
      "id": "ceADCyGmZBwr",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Detailed features: ',\n",
              " OrderedDict([('Petal.Width',\n",
              "               [{'id': 4,\n",
              "                 'name': 'Petal.Width',\n",
              "                 'operator': <OperatorCondition.GT: 52>,\n",
              "                 'sign': True,\n",
              "                 'operator_sign_considered': <OperatorCondition.LE: 51>,\n",
              "                 'threshold': 0.75,\n",
              "                 'weight': None,\n",
              "                 'theory': None,\n",
              "                 'string': 'Petal.Width <= 0.75'}])]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337056c0",
      "metadata": {
        "id": "337056c0"
      },
      "source": [
        "The ```to_features``` method eliminates redundant features by default and is also able to return more information about the features using the ```details``` parameter. This method is described in the [concepts](/documentation/explainer/concepts/) page.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d524a051",
      "metadata": {
        "id": "d524a051"
      },
      "source": [
        "We can check whether the derived explanation actually is a reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "21a1bd99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21a1bd99",
        "outputId": "d66662d2-8755-4495-9f0c-3a46bc5e003d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is sufficient:  True\n"
          ]
        }
      ],
      "source": [
        "print(\"is sufficient: \", explainer.is_sufficient_reason(sufficient_reason))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43404046",
      "metadata": {
        "id": "43404046"
      },
      "source": [
        "{: .attention }\n",
        "\n",
        "> It is important to note that computing and checking reasons are done independently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f5daf6",
      "metadata": {
        "id": "28f5daf6"
      },
      "source": [
        "To conclude, the sufficient reason (```('Petal.Width < 0.75',)```) explains why the instance ```[5.1 3.5 1.4 0.2]``` is well classified by the model (the prediction was Iris-setosa). It is because the fourth feature (the petal width in cm), set to 0.2 cm, is not greater or equal than 0.75 cm (see the attached image).\n",
        "<img src=\"attachment:irislatex.png\" alt=\"Iris\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f1f716",
      "metadata": {
        "id": "82f1f716"
      },
      "source": [
        "### Contrastive explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25efc49d",
      "metadata": {
        "id": "25efc49d"
      },
      "source": [
        "Now, let us consider another instance, a wrongly classified one using the parameter ```correct=False``` of the function ```get_instance```. We set this instance to the explainer with the ```set_instance``` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "c3887aae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3887aae",
        "outputId": "a68aa09a-c0ee-4b9c-8a26-ea729e39fcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------   Instances   ----------------\n",
            "number of instances selected: 1\n",
            "----------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "instance, prediction = learner.get_instances(model, n=1, correct=False)\n",
        "explainer.set_instance(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e38acce",
      "metadata": {
        "id": "9e38acce"
      },
      "source": [
        "We can explain why this instance is **not** classified differently by providing a contrastive explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "6c0ad185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c0ad185",
        "outputId": "d1f973d9-9e1c-4053-a252-d65b28b80ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contrastive reason (1,)\n",
            "to_features: ('Petal.Width > 0.75',)\n"
          ]
        }
      ],
      "source": [
        "contrastive_reason = explainer.contrastive_reason()\n",
        "print(\"contrastive reason\", contrastive_reason)\n",
        "print(\"to_features:\", explainer.to_features(contrastive_reason, contrastive=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b16200",
      "metadata": {
        "id": "d1b16200"
      },
      "source": [
        "More information about explanations can be found in the [Explainer Principles](/documentation/explainer/) page, the [Explaining Classification](/documentation/classification/) page and the [Explaining Regression](/documentation/regression/) page."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instances_with_prediction = learner.get_instances(model, n=10, indexes=Learning.TEST)\n",
        "for instance, prediction in instances_with_prediction:\n",
        "    print(\"instance:\", instance)\n",
        "    print(\"prediction\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG_xpbQZZnkH",
        "outputId": "ad7ee637-8e03-48f4-b7c4-82b8b7fb2483"
      },
      "id": "sG_xpbQZZnkH",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------   Instances   ----------------\n",
            "number of instances selected: 10\n",
            "----------------------------------------------\n",
            "instance: [5.8 2.8 5.1 2.4]\n",
            "prediction 2\n",
            "instance: [6.  2.2 4.  1. ]\n",
            "prediction 1\n",
            "instance: [5.5 4.2 1.4 0.2]\n",
            "prediction 0\n",
            "instance: [7.3 2.9 6.3 1.8]\n",
            "prediction 2\n",
            "instance: [5.  3.4 1.5 0.2]\n",
            "prediction 0\n",
            "instance: [6.3 3.3 6.  2.5]\n",
            "prediction 2\n",
            "instance: [5.  3.5 1.3 0.3]\n",
            "prediction 0\n",
            "instance: [6.7 3.1 4.7 1.5]\n",
            "prediction 1\n",
            "instance: [6.8 2.8 4.8 1.4]\n",
            "prediction 1\n",
            "instance: [6.1 2.8 4.  1.3]\n",
            "prediction 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Pièces jointes",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}